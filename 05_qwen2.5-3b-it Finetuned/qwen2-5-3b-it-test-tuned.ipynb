{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10168563,"sourceType":"datasetVersion","datasetId":6279747},{"sourceId":10177348,"sourceType":"datasetVersion","datasetId":6286164}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install peft\n!pip install trl\n!pip install torch\n!pip install bitsandbytes\n!pip install accelerate\n#!pip install wandb\n!pip install datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:58:03.047663Z","iopub.execute_input":"2024-12-12T08:58:03.048091Z","iopub.status.idle":"2024-12-12T08:59:06.085758Z","shell.execute_reply.started":"2024-12-12T08:58:03.048034Z","shell.execute_reply":"2024-12-12T08:59:06.084665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n#import wandb\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig, \n    TrainingArguments, \n    logging\n)\nfrom peft import LoraConfig, get_peft_model\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom trl import SFTTrainer, setup_chat_format\nimport bitsandbytes as bnb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")\nlogin(token=hf_token)\n\nbase_model = \"Qwen/Qwen2.5-3B-Instruct\"\nnew_model = \"qwen2.5-3b-it-tuned\"\n\nif torch.cuda.get_device_capability()[0] >= 8:\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\n    !pip install -qqq flash-attn  \nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:59:06.087655Z","iopub.execute_input":"2024-12-12T08:59:06.087955Z","iopub.status.idle":"2024-12-12T08:59:26.494489Z","shell.execute_reply.started":"2024-12-12T08:59:06.087928Z","shell.execute_reply":"2024-12-12T08:59:26.493775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:59:26.495509Z","iopub.execute_input":"2024-12-12T08:59:26.495813Z","iopub.status.idle":"2024-12-12T08:59:26.499867Z","shell.execute_reply.started":"2024-12-12T08:59:26.495785Z","shell.execute_reply":"2024-12-12T08:59:26.498856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:59:26.501501Z","iopub.execute_input":"2024-12-12T08:59:26.501756Z","iopub.status.idle":"2024-12-12T09:02:04.800388Z","shell.execute_reply.started":"2024-12-12T08:59:26.501732Z","shell.execute_reply":"2024-12-12T09:02:04.799591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=8,\n    task_type=\"CAUSAL_LM\",\n    target_modules= [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\n\ntokenizer.chat_template = None # Reset the chat template to prevent duplication error\n\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:02:04.801449Z","iopub.execute_input":"2024-12-12T09:02:04.801748Z","iopub.status.idle":"2024-12-12T09:02:05.29025Z","shell.execute_reply.started":"2024-12-12T09:02:04.801713Z","shell.execute_reply":"2024-12-12T09:02:05.28926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndataset = load_dataset(\"csv\",data_files= \"/kaggle/input/100q-training/combined_100_qna.csv\")\ndataset = dataset.shuffle(seed=42)\n\ndef clean_text(text):\n    \"\"\"Clean the input text by removing extra spaces and trimming.\"\"\"\n    return re.sub(r'\\s+', ' ', text).strip()\n\ndef custom_chat_template(batch):\n    \"\"\"Creates chat-style templates for each entry in the batch.\"\"\"\n    \n    # Clean the text fields in each batch\n    question = clean_text(batch['question'])\n    answer = clean_text(batch['answer'])\n    context = clean_text(batch['context'])\n    reasoning = clean_text(batch['reasoning'])\n\n    # Structure the data in a conversation format\n    row_json = [\n        {\"role\": \"user\", \"content\": question},\n        {\"role\": \"system\", \"content\": context},\n        {\"role\": \"assistant\", \"content\": f\"{answer} + ' ' + {reasoning}\"}\n    ]\n    \n    # Debug: Check the row_json structure\n    #print(\"Row JSON: \", row_json)\n\n    # Apply the tokenizer's chat template (assuming it handles JSON-like structures)\n    try:\n        batch['prompt'] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    except Exception as e:\n        print(\"Error in applying chat template:\", e)\n    \n    # Return the modified batch with the new 'prompt' field\n    return batch\n\nprint('~~~~~>')\nprint('Applying custom chat template')\nprint('~~~~~>')\n\n# Apply the custom chat template to each batch in the dataset\ndataset = dataset.map(custom_chat_template)\n\nprint('~~~~~>')\nprint('Success')\nprint('~~~~~>')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:02:05.291349Z","iopub.execute_input":"2024-12-12T09:02:05.291694Z","iopub.status.idle":"2024-12-12T09:02:05.783494Z","shell.execute_reply.started":"2024-12-12T09:02:05.291656Z","shell.execute_reply":"2024-12-12T09:02:05.782679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['train']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:02:05.784509Z","iopub.execute_input":"2024-12-12T09:02:05.784778Z","iopub.status.idle":"2024-12-12T09:02:05.790365Z","shell.execute_reply.started":"2024-12-12T09:02:05.784751Z","shell.execute_reply":"2024-12-12T09:02:05.789573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset[\"train\"],\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 1,\n        warmup_steps = 2,\n        max_steps = 15,\n        learning_rate = 2e-4,\n        fp16 = True,\n        logging_steps = 1,\n        do_eval=False,\n        output_dir = new_model,\n        optim = \"paged_adamw_8bit\"\n        \n    ),\n    peft_config = peft_config,\n    dataset_text_field = 'prompt',\n    tokenizer = tokenizer\n\n)\nmodel.config.use_cache = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:02:05.791331Z","iopub.execute_input":"2024-12-12T09:02:05.791546Z","iopub.status.idle":"2024-12-12T09:02:06.6818Z","shell.execute_reply.started":"2024-12-12T09:02:05.791524Z","shell.execute_reply":"2024-12-12T09:02:06.680856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:02:06.683197Z","iopub.execute_input":"2024-12-12T09:02:06.683573Z","iopub.status.idle":"2024-12-12T09:02:29.85255Z","shell.execute_reply.started":"2024-12-12T09:02:06.683531Z","shell.execute_reply":"2024-12-12T09:02:29.851687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clear all files and directories in /kaggle/working\n!rm -rf /kaggle/working/*\n\ntrainer.model.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)\n\ntrainer.model.save_pretrained('./my_model')\ntokenizer.save_pretrained('./my_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:02:29.855791Z","iopub.execute_input":"2024-12-12T09:02:29.856107Z","iopub.status.idle":"2024-12-12T09:02:39.079044Z","shell.execute_reply.started":"2024-12-12T09:02:29.856077Z","shell.execute_reply":"2024-12-12T09:02:39.077989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\n\ntokenizer = AutoTokenizer.from_pretrained(new_model, trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.resize_token_embeddings(len(tokenizer))\n\nmodel = PeftModel.from_pretrained(model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:02:39.080427Z","iopub.execute_input":"2024-12-12T09:02:39.080747Z","iopub.status.idle":"2024-12-12T09:03:02.891989Z","shell.execute_reply.started":"2024-12-12T09:02:39.080716Z","shell.execute_reply":"2024-12-12T09:03:02.891152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"message = [\n    {\"role\": \"user\", \"content\": \"What is the GSTIN of the company billing to Manjrekar & Sons?\"},\n    {\"role\": \"system\", \"content\": '{\"Key_value_data\": [{\"Invoice No.\": \"RTR354\", \"Invoice Date\": \"2-Dec-20\", \"Invoice From-Company Name\": \"National Enterprises\", \"Invoice From-Company GSTIN\": \"29AACCT3705E000\", \"Invoice From-Company Email\": \"\", \"Invoice From-Company Phone No.\": \"\", \"Invoice From-Company Address\": \"HSR Layout\\nBangalore\", \"Invoice To-Company Name\": \"Manjrekar & Sons\", \"Invoice To-Company GSTIN\": \"27BJPJK0301P1ZT\", \"Invoice To-Company Email\": \"\", \"Invoice To-Company Phone No.\": \"\", \"Invoice To-Company Address\": \" \", \"Gross Amount\": \"500.00\", \"Tax Amount\": \"60.00\", \"Discount\": \"\", \"Total Invoice Amount\": \"560.00\"}], \"Table_data_1\": [{\"S No.\": \"1\", \"Description\": \"12MM**\", \"HSN/SAC\": \"1004\", \"Quantity\": \"10\", \"Rate\": \"50.00\", \"Unit\": \"No\", \"Amount\": \"500.00\"}], \"Table_data_2\": [{\"HSN/SAC\": \"1004\", \"Value of Supply\": \"\", \"Taxable Value\": \"500.00\", \"Central Tax Amount\": \"\", \"State Tax Amount\": \"\", \"Total Tax Amount\": \"60.00\"}]}'}\n]\n\nprompt = tokenizer.apply_chat_template(message, tokenize = False, add_generation_prompt = True)\ninputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\noutputs = model.generate(**inputs, max_new_tokens = 100)\n\nresponse = tokenizer.decode(outputs[0]).split('assistant')[-1].strip()\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:03:02.895448Z","iopub.execute_input":"2024-12-12T09:03:02.895761Z","iopub.status.idle":"2024-12-12T09:03:06.328729Z","shell.execute_reply.started":"2024-12-12T09:03:02.895732Z","shell.execute_reply":"2024-12-12T09:03:06.327832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/inference-test-set/inference-test-set.csv')\nquestions_df = df['question'] # Assumes the file has a 'Question' column\n\ncontext = '{\"Key_value_data\": [{\"Invoice No.\": \"4545 / 2017-18\", \"Invoice Date\": \"15/09/2017\", \"Invoice From-Company Name\": \"BLUE SKY INDIA LIMITED\", \"Invoice From-Company GSTIN\": \"078DFGHJ412421SF\", \"Invoice From-Company Email\": \"test@gmail.com\", \"Invoice From-Company Phone No.\": \"+91645641236485\", \"Invoice From-Company Address\": \"Plot No. 44, Sector-20, Dwarka- 110075\", \"Invoice To-Company Name\": \"TechGuruPlus\", \"Invoice To-Company GSTIN\": \"656564566345454\", \"Invoice To-Company Email\": \"\", \"Invoice To-Company Phone No.\": \"01356656565\", \"Invoice To-Company Address\": \"C-172, Okhla Industrial Area, \\nNew Delhi-110020\", \"Gross Amount\": \"10960.00\", \"Tax Amount\": \"1952.00\", \"Discount\": \"\", \"Total Invoice Amount\": \"12932.00\"}], \"Table_data_1\": [{\"S No.\": \"45\", \"Description\": \"ITEM NAME 1\", \"HSN/SAC\": \"4556\", \"Quantity\": \"20.00\", \"Rate\": \"\", \"Unit\": \"\", \"Amount\": \"900.00\"}, {\"S No.\": \"23\", \"Description\": \"ITEM NAME 2\", \"HSN/SAC\": \"8978\", \"Quantity\": \"40.00\", \"Rate\": \"\", \"Unit\": \"\", \"Amount\": \"920.00\"}, {\"S No.\": \"56\", \"Description\": \"ITEM NAME 3\", \"HSN/SAC\": \"5645\", \"Quantity\": \"50.00\", \"Rate\": \"\", \"Unit\": \"\", \"Amount\": \"2800.00\"}, {\"S No.\": \"89\", \"Description\": \"ITEM NAME 4\", \"HSN/SAC\": \"2312\", \"Quantity\": \"60.00\", \"Rate\": \"\", \"Unit\": \"\", \"Amount\": \"5340.00\"}]}'\n\nresponses = []\n\n# Iterate over each question and generate a response\nfor question in questions_df:\n    \n    \n    # Prepare the message for the model\n    message = [\n        {\"role\": \"user\", \"content\": question},\n        {\"role\": \"system\", \"content\": context}\n    ]\n\n    # Format the prompt using the chat template\n    prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n\n    # Tokenize and run the model\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=100)\n\n    # Decode the model's output\n    response = tokenizer.decode(outputs[0]).split('assistant')[-1].strip()\n\n    # Append the response to the list\n    responses.append(response)\n\noutput_df = pd.DataFrame({\n    'question': questions_df,\n    'Model Response': responses\n})\n# Save the updated DataFrame to a new Excel/CSV file\noutput_df.to_excel('qwen_2.5_3b_it_responses.xlsx', index=False)\n\nprint(\"Responses sheet has been generated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T09:03:06.330353Z","iopub.execute_input":"2024-12-12T09:03:06.330891Z","iopub.status.idle":"2024-12-12T09:03:49.92181Z","shell.execute_reply.started":"2024-12-12T09:03:06.330846Z","shell.execute_reply":"2024-12-12T09:03:49.920871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}